{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eaf0e0f",
   "metadata": {},
   "source": [
    "#  Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15d8d7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                image label\n",
      "0  Img/img001-001.png     0\n",
      "1  Img/img001-002.png     0\n",
      "2  Img/img001-003.png     0\n",
      "3  Img/img001-004.png     0\n",
      "4  Img/img001-005.png     0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "csv_path = r\"D:\\New folder\\english.csv\"\n",
    "image_dir = r\"D:\\New folder\\Img\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(df.head()) # Take a look at the structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2f1705",
   "metadata": {},
   "source": [
    "# Extract image names and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ecf5951",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = df['image'].tolist() # Assuming a column named 'image_name'\n",
    "labels = df['label'].tolist()     # Assuming a column named 'label_code'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71eba09",
   "metadata": {},
   "source": [
    "# Load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e169d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "for img_file in image_files:\n",
    "    # Check if the filename starts with \"Img/\" and remove it if it does\n",
    "    if img_file.startswith(\"Img/\"):\n",
    "        base_filename = img_file[len(\"Img/\"):]\n",
    "    else:\n",
    "        base_filename = img_file\n",
    "\n",
    "    # Now join the base filename with the image directory\n",
    "    img_path = os.path.join(image_dir, base_filename)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) # Load as grayscale\n",
    "    if img is not None:\n",
    "        # Resize the image (adjust dimensions as needed for your model)\n",
    "        resized_img = cv2.resize(img, (64, 64))\n",
    "        # Normalize pixel values\n",
    "        normalized_img = resized_img / 255.0\n",
    "        image_list.append(normalized_img)\n",
    "    else:\n",
    "        print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "# Convert the list of images to a NumPy array\n",
    "X = np.array(image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea55962c",
   "metadata": {},
   "source": [
    "# Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a114b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels) # Convert string labels to numerical\n",
    "# If your model requires one-hot encoding:\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6beb7",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1e16587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86172f0",
   "metadata": {},
   "source": [
    "# Build and Train Your Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05a9180a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Piyush\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.0385 - loss: 4.0751 - val_accuracy: 0.1465 - val_loss: 3.5015\n",
      "Epoch 2/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.3161 - loss: 2.7202 - val_accuracy: 0.3590 - val_loss: 2.5778\n",
      "Epoch 3/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.5845 - loss: 1.5860 - val_accuracy: 0.4322 - val_loss: 2.3242\n",
      "Epoch 4/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.7441 - loss: 0.9440 - val_accuracy: 0.5092 - val_loss: 2.1381\n",
      "Epoch 5/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8448 - loss: 0.5545 - val_accuracy: 0.5385 - val_loss: 2.1371\n",
      "Epoch 6/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.9230 - loss: 0.3178 - val_accuracy: 0.5385 - val_loss: 2.2648\n",
      "Epoch 7/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9572 - loss: 0.1759 - val_accuracy: 0.5238 - val_loss: 2.5270\n",
      "Epoch 8/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9757 - loss: 0.1260 - val_accuracy: 0.5385 - val_loss: 2.6236\n",
      "Epoch 9/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9828 - loss: 0.0752 - val_accuracy: 0.5348 - val_loss: 2.7163\n",
      "Epoch 10/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.9893 - loss: 0.0419 - val_accuracy: 0.5495 - val_loss: 2.9010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x28ca0967580>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Reshape the input data for CNN (assuming grayscale images)\n",
    "X_train = X_train.reshape(-1, 64, 64, 1)\n",
    "X_test = X_test.reshape(-1, 64, 64, 1)\n",
    "\n",
    "num_classes = len(np.unique(y)) # Determine the number of unique classes\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax') # Softmax for multi-class classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', # Or 'categorical_crossentropy' if using one-hot encoding\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10 # Adjust as needed\n",
    "model.fit(X_train, y_train, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d01ab",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37d7cb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5996 - loss: 2.8216\n",
      "Test loss: 2.6705\n",
      "Test accuracy: 0.6041\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f5262",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "685331ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pillow\n",
      "  Downloading pillow-11.2.1-cp310-cp310-win_amd64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\piyush\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytesseract) (25.0)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading pillow-11.2.1-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.7 MB 621.2 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.5/2.7 MB 621.2 kB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 0.8/2.7 MB 657.8 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 1.0/2.7 MB 739.8 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 1.6/2.7 MB 975.2 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.8/2.7 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.4/2.7 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pillow, pytesseract\n",
      "\n",
      "   ---------------------------------------- 0/2 [pillow]\n",
      "   ---------------------------------------- 0/2 [pillow]\n",
      "   ---------------------------------------- 0/2 [pillow]\n",
      "   ---------------------------------------- 0/2 [pillow]\n",
      "   ---------------------------------------- 0/2 [pillow]\n",
      "   ---------------------------------------- 0/2 [pillow]\n",
      "   ---------------------------------------- 0/2 [pillow]\n",
      "   ---------------------------------------- 0/2 [pillow]\n",
      "   ---------------------------------------- 0/2 [pillow]\n",
      "   ---------------------------------------- 0/2 [pillow]\n",
      "   ---------------------------------------- 0/2 [pillow]\n",
      "   ---------------------------------------- 0/2 [pillow]\n",
      "   -------------------- ------------------- 1/2 [pytesseract]\n",
      "   -------------------- ------------------- 1/2 [pytesseract]\n",
      "   ---------------------------------------- 2/2 [pytesseract]\n",
      "\n",
      "Successfully installed pillow-11.2.1 pytesseract-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script pytesseract.exe is installed in 'c:\\Users\\Piyush\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install pytesseract pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77924ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word: 3)\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# For Windows: specify path to tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # <- Update this path if different\n",
    "\n",
    "# Set the path to the image\n",
    "image_path = \"Img\\img062-055.png\"\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Use pytesseract to extract text\n",
    "predicted_text = pytesseract.image_to_string(image, config='--psm 8')\n",
    "\n",
    "# Clean and print the result\n",
    "predicted_text = predicted_text.strip()\n",
    "print(\"Predicted word:\", predicted_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8879d96",
   "metadata": {},
   "source": [
    "# WER and CER test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a8ee430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|██████████| 3410/3410 [1:15:42<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Final WER on test set: 1.0249\n",
      "Final CER on test set: 0.9590\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, TrOCRProcessor\n",
    "from jiwer import wer, cer\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load model and processor\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Set the path to the folder containing test images\n",
    "image_folder = r\"D:\\New folder\\Img\"\n",
    "image_files = sorted([\n",
    "    f for f in os.listdir(image_folder)\n",
    "    if f.endswith(('.png', '.jpg', '.jpeg'))\n",
    "])\n",
    "\n",
    "predictions = []\n",
    "ground_truths = []  # You must manually fill this or load from file\n",
    "\n",
    "for img_file in tqdm(image_files, desc=\"Evaluating\"):\n",
    "    img_path = os.path.join(image_folder, img_file)\n",
    "\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "        pred_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    predictions.append(pred_text)\n",
    "\n",
    "    # ⚠️ Replace with actual ground truth for real evaluation\n",
    "    ground_truths.append(\"actual ground truth text here\")\n",
    "\n",
    "# Compute final WER and CER\n",
    "final_wer = wer(ground_truths, predictions)\n",
    "final_cer = cer(ground_truths, predictions)\n",
    "\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Final WER on test set: {final_wer:.4f}\")\n",
    "print(f\"Final CER on test set: {final_cer:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
